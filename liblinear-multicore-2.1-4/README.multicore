Introduction
============

This extension of liblinear supports multi-core parallel learning by OpenMP.
We applied efficient implementations to train models for multi-core machines.
Currently, -s 0 (solving primal LR), -s 2 (solving primal l2-loss SVM),
-s 1 (solving dual l2-loss SVM), -s 3 (solving dual l1-loss SVM),
and -s 11 (solving primal l2-loss SVR) are supported.

Usage
=====

The usage is the same as liblinear except for the additional option:

-n nr_thread: use nr_thread threads for training (only for -s 0, -s 1, -s 2, -s 3 and -s 11)

Examples
========

> ./train -s 0 -n 8 heart_scale

will run the L2-regularized logistic regression primal solver with 8 threads.

> ./train -s 3 -n 8 heart_scale

will run the L2-regularized l1-loss dual solver with 8 threads.

Differences from LIBLINEAR
==========================

The major changes for the primal solver are to do matrix-vector multiplications in
parallel. The major changes for the dual solver are to calculate the gradients
in parallel, and then do CD (coordinate descent) updates serially.
Details could be seen in the references.

Experiment Results
==================

Primal Logistic Regression:
dataset              1-thread  4-threads   8-threads
covtype_scale           1.824      0.718       0.684
epsilon_normalized    111.906     29.958      17.920
rcv1_test.binary       15.157      4.218       2.507
webspam(trigram)      509.456    185.467     117.229

Dual l1-loss SVM:
dataset              1-thread  4-threads   8-threads
covtype_scale           4.624      2.935       2.330
epsilon_normalized     19.951      8.444       7.514
rcv1_test.binary        2.369      1.071       0.856
webspam(trigram)       64.064     24.065      17.216

Dual l2-loss SVM:
dataset              1-thread  4-threads   8-threads
covtype_scale           7.736      5.355       5.193
epsilon_normalized     28.766     18.202      17.282
rcv1_test.binary        2.910      1.736       1.466
webspam(trigram)       41.6925    21.883      16.794

Note that the primal and dual results are not comparable because they
solve different problems.

Reference
=========

M.-C. Lee, W.-L. Chiang, and C.-J. Lin.  
Fast matrix-vector multiplications for large-scale logistic regression on shared-memory systems.  
ICDM 2015

W.-L. Chiang, M.-C. Lee, and C.-J. Lin.
Parallel dual coordinate descent method for large-scale linear classification in multi-core environments.
Technical report, National Taiwan University, 2016

For any questions and comments, please email
cjlin@csie.ntu.edu.tw
